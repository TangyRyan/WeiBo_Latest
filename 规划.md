• 整体方案

  - 以“采集→理解→健康专题加工→API 服务”四层架构落地，所有数据沿用本地 JSON 存储（backend/storage.py:19-120）以满足无数据库约束。
  - 健康专题数据来源于 LLM 事件理解结果（backend/llm/analysis.py:56-75）；当 topic_type=="健康" 时要求返回 health_major/health_minor（健康.md:1-13），并为后续可视化准备结构化
    素材。
  - 方案需输出两大接口：一个时间轴列表，一个单事件详情。timeline 里只保留 category（对应 health_major 或已存在的 nine-class key），无需 category_color，颜色由前端自行映射。

  数据流与模块

  - 采集层：延续现有爬虫/调度，将热度与帖子写入 data/hotlist, data/archive 等，接口通过 storage.read_json/write_json 管理。
  - 事件理解层：backend/llm/analysis.py 新增健康专用 prompt/解析器。流程：初判为健康 → 二次 prompt 获取 health_major/health_minor + 情绪向量 → 合并入事件档案（data/archive/
    {date}.json），并保留 hot_values, tags, posts 等原字段。
  - 健康专题加工（新建 backend/health/ 包）
      1. models.py: 定义 HealthEvent, TimelinePoint, EventDetail dataclass，保证 JSON schema 稳定。
      2. ingest.py: 读取最近 48h 或指定窗口的 archive，筛出健康事件，补充默认字段（如缺少情绪则置 0）。
      3. timeline.py: 将每个事件的 hot_values 归一至 10 分钟粒度，生成 [{event_id,title,start_ts,end_ts,category,point_count,points:[{ts,heat,rank}]}]；附加汇总统计（总事件数、
         各类别计数）以满足前端右上角需求（健康.md:5-8）。
      4. features.py:
          - 标签共现：依据事件 tags/命名实体构图，python-louvain 求社区，输出 nodes（含 id, label, community）与 edges（source, target, weight）。
          - 词云：整合同事件的热门贴文，jieba 分词，去停用词与低频词，返回 [{text, weight}]。
          - 情绪：直接读取 LLM 返回的 8 维向量，缺失值补零，保持 "喜悦"~"期盼" 顺序（健康.md:11-13）。
      5. serializer.py: 负责把 timeline 写入 data/health/timeline/latest.json；事件详情写入 data/health/events/{date}/{event_id}.json；还维护 data/health/index.json（可用日期
         列表）。
  - 调度层：在 backend/scheduler.py 中注册 health_topic_job（依赖 APScheduler）；每 10 分钟执行：
      1. 加锁 data/health/.lock；
      2. 调用 ingest→timeline→features→serializer；
      3. 原子替换 latest.json，若失败保留旧版。晚间再将 latest.json 归档到 data/health/archive/{date}/timeline.json。
  - 服务层（Flask 蓝图）：新增 backend/health/api.py 并在 backend/app.py 注册。接口：
      1. GET /api/health/timeline?date=&hours= → 返回 timeline 列表、类别计数与 updated_at。
      2. GET /api/health/events/<event_id>?date= → 返回事件详情（基础信息、标签网络、词云、情绪、样本帖子摘要）。
      3. GET /api/health/dates → 返回 index.json 里的日期数组。
         所有接口直接读取 JSON，利用现有 CORS/WebSocket 配置（backend/app.py:17-44）即可。
  - 文件结构建议：

    data/
      health/
        timeline/latest.json
        events/YYYY-MM-DD/{event_id}.json
        archive/YYYY-MM-DD/{timeline,events}.json
        index.json
     示例：

    {
      "updated_at": "2025-11-13T10:20:00+08:00",
      "summary": {"total_events": 18, "by_major": {"医疗服务与医患矛盾": 5, ...}},
      "events": [
        {
          "event_id": "20251113-123",
          "title": "××医院护士被袭事件",
          "category": "医疗服务与医患矛盾",
          "start_ts": 1731460800,
          "end_ts": 1731471600,
          "heat_peak": 985000,
          "points": [{"ts":1731460800,"heat":120000,"rank":48}, ...],
          "sentiment": -0.35,
          "region": "广东"
        }
      ]
    }

  实现与测试建议

  - 先完成 models.py 与 serializer.py，构造静态样例供前端联调。
  - 离线跑 features.py/timeline.py，使用历史 JSON 验证社区/词云/情绪输出。
  - 引入新依赖（jieba, python-louvain, networkx, numpy）写入 requirements.txt，由开发者线下安装。
  - 编写最小单元测试或 CLI（scripts/gen_health_snapshot.py）模拟 scheduler 逻辑，确保 JSON 生成可重复。
  - 接口完成后，用 curl/pytest 校验 3 个 API 的响应结构，再交付给前端同学消费。

  若需要，我可以继续细化数据模型或先生成样例 JSON。• 整体方案

  - 以“采集→理解→健康专题加工→API 服务”四层架构落地，所有数据沿用本地 JSON 存储（backend/storage.py:19-120）以满足无数据库约束。
  - 健康专题数据来源于 LLM 事件理解结果（backend/llm/analysis.py:56-75）；当 topic_type=="健康" 时要求返回 health_major/health_minor（健康.md:1-13），并为后续可视化准备结构化
    素材。
  - 方案需输出两大接口：一个时间轴列表，一个单事件详情。timeline 里只保留 category（对应 health_major 或已存在的 nine-class key），无需 category_color，颜色由前端自行映射。

  数据流与模块

  - 采集层：延续现有爬虫/调度，将热度与帖子写入 data/hotlist, data/archive 等，接口通过 storage.read_json/write_json 管理。
  - 事件理解层：backend/llm/analysis.py 新增健康专用 prompt/解析器。流程：初判为健康 → 二次 prompt 获取 health_major/health_minor + 情绪向量 → 合并入事件档案（data/archive/
    {date}.json），并保留 hot_values, tags, posts 等原字段。
  - 健康专题加工（新建 backend/health/ 包）
      1. models.py: 定义 HealthEvent, TimelinePoint, EventDetail dataclass，保证 JSON schema 稳定。
      2. ingest.py: 读取最近 48h 或指定窗口的 archive，筛出健康事件，补充默认字段（如缺少情绪则置 0）。
      3. timeline.py: 将每个事件的 hot_values 归一至 10 分钟粒度，生成 [{event_id,title,start_ts,end_ts,category,point_count,points:[{ts,heat,rank}]}]；附加汇总统计（总事件数、
         各类别计数）以满足前端右上角需求（健康.md:5-8）。
      4. features.py:
          - 标签共现：依据事件 tags/命名实体构图，python-louvain 求社区，输出 nodes（含 id, label, community）与 edges（source, target, weight）。
          - 词云：整合同事件的热门贴文，jieba 分词，去停用词与低频词，返回 [{text, weight}]。
          - 情绪：直接读取 LLM 返回的 8 维向量，缺失值补零，保持 "喜悦"~"期盼" 顺序（健康.md:11-13）。
      5. serializer.py: 负责把 timeline 写入 data/health/timeline/latest.json；事件详情写入 data/health/events/{date}/{event_id}.json；还维护 data/health/index.json（可用日期
         列表）。
  - 调度层：在 backend/scheduler.py 中注册 health_topic_job（依赖 APScheduler）；每 10 分钟执行：
      1. 加锁 data/health/.lock；
      2. 调用 ingest→timeline→features→serializer；
      3. 原子替换 latest.json，若失败保留旧版。晚间再将 latest.json 归档到 data/health/archive/{date}/timeline.json。
  - 服务层（Flask 蓝图）：新增 backend/health/api.py 并在 backend/app.py 注册。接口：
      1. GET /api/health/timeline?date=&hours= → 返回 timeline 列表、类别计数与 updated_at。
      2. GET /api/health/events/<event_id>?date= → 返回事件详情（基础信息、标签网络、词云、情绪、样本帖子摘要）。
      3. GET /api/health/dates → 返回 index.json 里的日期数组。
         所有接口直接读取 JSON，利用现有 CORS/WebSocket 配置（backend/app.py:17-44）即可。
  - 文件结构建议：

    data/
      health/
        timeline/latest.json
        events/YYYY-MM-DD/{event_id}.json
        archive/YYYY-MM-DD/{timeline,events}.json
        index.json
     示例：

    {
      "updated_at": "2025-11-13T10:20:00+08:00",
      "summary": {"total_events": 18, "by_major": {"医疗服务与医患矛盾": 5, ...}},
      "events": [
        {
          "event_id": "20251113-123",
          "title": "××医院护士被袭事件",
          "category": "医疗服务与医患矛盾",
          "start_ts": 1731460800,
          "end_ts": 1731471600,
          "heat_peak": 985000,
          "points": [{"ts":1731460800,"heat":120000,"rank":48}, ...],
          "sentiment": -0.35,
          "region": "广东"
        }
      ]
    }

  实现与测试建议

  - 先完成 models.py 与 serializer.py，构造静态样例供前端联调。
  - 离线跑 features.py/timeline.py，使用历史 JSON 验证社区/词云/情绪输出。
  - 引入新依赖（jieba, python-louvain, networkx, numpy）写入 requirements.txt，由开发者线下安装。
  - 编写最小单元测试或 CLI（scripts/gen_health_snapshot.py）模拟 scheduler 逻辑，确保 JSON 生成可重复。
  - 接口完成后，用 curl/pytest 校验 3 个 API 的响应结构，再交付给前端同学消费。

  若需要，我可以继续细化数据模型或先生成样例 JSON。