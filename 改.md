• 合并思路

  - 统一后端入口：沿用 weibo_llm/backend/app.py:19-170 的 Flask 工厂，新增一个 Blueprint 将 spider/hot_topics_api.py:503-1078 注册进来，使 /api/hot_topics/*（模块1-2）与 /api/daily_30、/api/risk/*、/api/central_data 及 /ws/*（模块3-6）由同一进程
    提供。
  - 调度整合：在同一个 BackgroundScheduler 中调度 spider/monitor_remote_hot_topics.py:171-318（24小时抓取/AI Card/帖子缓存）和 weibo_llm/backend/scheduler.py:53-207（小时归档、LLM 风险评估、推送），保持原有任务间依赖顺序。

  目录与路径

  - 新建统一 DATA_ROOT（默认为项目根下 data/），通过环境变量 WEIBO_DATA_ROOT 提供；spider 与 backend 的 ARCHIVE_DIR、HOTLIST_DIR、RISK_DIR、POST_DIR、AICARD_DIR 都引用该根目录。
  - 将 backend/config.py:4-43 指向与 spider/daily_heat.py:16-24 相同的目录结构（data/hot_topics, data/hot_topics/hourly, data/posts, data/aicard, data/daily_bundles），同时废弃 weibo_llm/backend/data 中的重复存储。
  - Spider 返回的路径（如 html_path/json_path）保持 DATA_ROOT 相对路径，分析层可直接用 Path(DATA_ROOT) / rel_path 读取。

  接口映射（规划.md 六模块）

  1. 当前热榜：GET /api/hot_topics/hourly（spider/hot_topics_api.py:810-838）输出 {date, hour, topics:[{rank,title,hot,slug}]}；/api/hotlist/current（backend/app.py:75-85）读取同一小时文件供顶部列表使用。后续用D:\LZY_Project\Weibo_1106\spider\hot_topics_ws.py ws与前端交互
  2. 事件概览/贴文：GET /api/hot_topics/aicard + /api/hot_topics/posts（spider/hot_topics_api.py:839-1048）提供总结 HTML 与帖子；点击热榜条目时联动调用。
  3. 热度|风险 30 天：GET /api/daily_30（backend/app.py:55-74）聚合 archive 中每日 heat_total/risk_total，满足轮播需求。
  4. 风险预警：GET /api/risk/latest 与 /ws/risk_warnings（app.py:80-137）（websocket）输出 top_risk_warnings（scheduler.py:208-247）结果；模块 4 点击后驱动模块 5。
  5. 风险研判：新增 GET /api/risk/event?name=...&date=...，直接从 archive[date].json 中返回 risk_dims、llm、最新帖子摘要，用于展示维度评分 + 文字说明。
  6. 中央分布：GET /api/central_data?range=week|month|halfyear&color=field（app.py:101-124）已提供 {name,date,领域,地区,情绪,风险}；前端仅需把筛选器映射到 range/color 参数。

  接口调用关系

  - 将 backend/fetchers/classmate_adapter.py:6-90 改成“本地优先 + HTTP 备份”模式：优先通过 storage.load_hour_hotlist 和 spider.update_posts.ensure_topic_posts 读本地文件；若缺失再请求外部接口，便于单机部署。
  - daily_llm_update（scheduler.py:135-205）读取 data/posts/{date}/{slug}.json 缓存作为 posts 输入，替换原来的 HTTP 请求，得到统一的 {published_at, account_name, content_text, media, reposts, comments, likes} 结构。
  - process_hour_hotlist 保存小时榜后直接复用 spider 生成的 hourly JSON，避免重复定义；生成的 archive 字段（hot_values/hour_list/llm/risk_dims）供全部模块共享。
  - Spider 中记录的 AI Card 信息（aicard 字段）在 LLM 更新时写回 archive，模块 2、5 通过统一事件对象获取。

  实施步骤

  1. 抽取公共配置（backend/settings.py），把 .env 加载逻辑和 DATA_ROOT 变量共享到 spider/backend。
  2. 将 spider 目录改为包式引用（如 from backend.collector.update_posts import ensure_topic_posts），确保相对导入统一。
  3. 调整存储/适配器函数，使爬虫输出与分析调度共用同一读写 API。
  4. 在 Flask app 中注册 spider Blueprint，验证 /api/hot_topics/* 与 /api/daily_30 等接口共存，并测试 WebSocket。
  5. 更新 scripts/run_server.py 为“一键启动调度+后端”，完成六个模块数据流闭环。